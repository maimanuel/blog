{
  
    
        "post0": {
            "title": "Decision Trees from Scratch",
            "content": "Introduction . Decision Trees form an important basis for one of the most popular and useful supervised machine learning algorithms of modern times, the Random Forest. In this post I will develop a very basic, non-optimized version of a decision tree for regression. I will optimize the tree and build a random forest from multiple trees in a later post. To say this post is inspired by fastai&#39;s machine learning course is an understatement. I have watched the videos and follow Jeremy&#39;s code closely. . How Does it Work? - Predict the Mean! . The goal of any supervised machine learning algorithm is to use a set of input features $X$ and map them as closely to a set of targets $y$. In other words, the task is to find a function $f$ such that $$f(X) approx y.$$ A decision tree is one of many algorithms that allow us to effectively find a good function $f$. The basic idea is the following. If we wanted to create a trivial model that predicted the exact same value for all rows of $X$, which value should we choose? That depends on how we measure how close our prediction is to the actual targets $y$! It is very common to try to minimize the mse or mean squared error of our prediction $f(X)$. The means squared error is defined as $${ rm MSE}(y^{ rm true},y^{ rm pred}) = frac{1}{n} sum_{i=1}^{n}(y_i^{ rm true} -y_i^{ rm pred})^2,$$ . where the predictions are the model output $y^{ rm pred} = f(X)$. If we build a model where all our predictions are the same, there is one specific value that would minimize the mean squared error and that value is the mean of the true labels for every single row of $X$. . Where is the Tree? . Predicting the mean of the target variable is a good approximation, but we can obviously do better. We will now do split the rows into two groups, and then predict the mean of $y^{ rm true}$ for each of the separate groups. And then we repeat on the subgroups and split those again, and so on... . Why should that be better? Well, consider the case were we have split the rows to the point where each row gets their own prediction. In that case there would be no error at all, but the model would be very specific to the dataset, i.e., if you are row $i$, we predict $y_i^{ rm true}$. This model would be overfit to the data. . In general, we want to be somewhere in between the extremes, predict more than one value for the entire dataset, but predict fewer values than the exact ones that are given in the training set. In order to get there, we split the data some number of times, how often is controlled by a hyperparameter of the model. . The decision tree is hierarchical structure of splits that divide the dataset into groups, such that each element of the new groups can be better approximated by the group mean of targets, than by the mean of targets of the combined groups before the split. . How to Choose the Splits? . To find the splits we will simply iterate through all columns and for each column check the range of all values in the columns as a cutoff point. For example, assume we want to predict a persons weight from the two columns age and height. First, check column age, and try out all possible values of ages that are found in the column. . Let&#39;s say there are values 10, 55, 68. For each of those values check if using the mean weight of the group produces a lower error than predicting just one weight for everyone. In this case there are only two groupings available, everyone with age $ leq 10$ and everyone with age $ leq 55$. It seems likely, that the persons with age 55, and 68 have a weight closer to each other than a child (or several children) of age 10. If the suspicion is true, we will keep variable age and split value 55 as candidates for the first split. . Next up, we will check all values of the column height and try to achieve a lower error than our candidate split from column age. If we cannot find a split that produces a lower error, we will keep the candidate split, otherwise we will use the newly found better one. We can now repeat that procedure with the same columns on the subgroups, until we hit a stopping criterion. This could be max number of splits = tree depth has bee reached, or we would require a minimum number of rows per group and we cannot split anymore if we would have too few rows in a group. . This is basically the entire algorithm! Once we have set up the tree with all the split columns and values, we can send new and unseen data through the tree, e.g., if age &lt;= 55 go right, if height &gt; 5&#39;8&#39;&#39; go left then predict group mean weight = 185lbs. . Let&#39;s Code Up a Basic Version . First lines of code to initialize the base class. . class DecisionTree(): def __init__(self,x,y,idxs=None,min_leaf_size=5,max_tree_depth=3): self.x,self.y,self.idxs,self.min_leaf_size,self.max_tree_depth = x,y,idxs,min_leaf_size,max_tree_depth . To initialize a tree we will pass it some basic information. The data $x$, in our case a pandas DataFrame, the prediction target, or labels, $y$ which will be a numpy array, and possibly some other parameters, such as min_leaf_size, and max_tree_depth. In case we are not at the root node, we will also need to pass a set of indices to refer to the rows, which made it into this particular node a after previous split. . If we are at the root node, then the rows to consider to split on are actually all rows. Hence we can simply construct a full set of idxs in case the value is None. . class DecisionTree(): def __init__(self,x,y,idxs=None,min_leaf_size=5,max_tree_depth=3): self.x,self.y,self.min_leaf_size,self.max_tree_depth = x,y,min_leaf_size,max_tree_depth self.idxs = np.arange(len(y)) if idxs is None else idxs self.n_rows, self.n_cols = len(self.idxs),self.x.shape[1] self.val = np.mean(self.y[self.idxs]) self.score = float(&#39;inf&#39;) self.find_varsplit() . Next up is the computation of the prediction value for this node, which is simply the mean of all $y$s that made it to the node. Since we want to minimize the error score, we&#39;ll also keep track of that and initialize it to $ infty$. Instead of the MSE, we&#39;re tracking the total sum of squares, to go around thinking about computing weighted averages. Finally, we want to find out for on which variable (if any) we will split next, and at which lavel of that variable we will split the remaining data. . Let&#39;s try to figure out how to do the splitting. . class DecisionTree(): def __init__(self,x,y,idxs=None,min_leaf_size=5,max_tree_depth=3): ... self.find_varsplit() #go through all variables and find a variable split def find_varsplit(self): for i in range(n_cols): self.find_better_split(i) # find a split that achieves a lower score than the one we already have def find_better_split(self,i): pass . The idea here is that once we are sure that we want another split, we iterate through all variables (columns), find the best split for that variable, and compare that to self.score. If the newly found best split on variable i is better than the new best one so far, we&#39;ll keep it as the best, otherwise we move on. . def sum_squares(x_vals,x_sq): return np.sum(x_sq - np.mean(x_vals)**2) def find_better_split(self,i): y = self.y[self.idxs] y_sq = y*y col_data = self.x.iloc[self.idxs,i] for x_val in self.x.iloc[self.idxs,i].unique(): left_ss,right_ss = 0.,0. f_left = col_data &lt;= x_val f_right = col_data &gt; x_val if np.sum(f_left) &gt; 0: left_ss = sum_squares(y[f_left],y_sq[f_left]) if np.sum(f_right) &gt; 0: right_ss = sum_squares(y[f_right],y_sq[f_right]) if self.score &gt; left_ss+right_ss: self.score,self.var,self.split = left_ss+right_ss,i,x_val self.left_idxs,self.right_idxs = self.idxs[f_left],self.idxs[f_right] . This function goes through a particular column, indexed by i, and splits the data by all possible unique values that are found in the column. For each of the splits, left and right, we compute the sum of squared errors when predicting the group mean of the target variable. We then check if this is the best split found so far. If it is, we keep the relevant information, indices for left and right groups, column index, and split value. . Putting it All Together . Now we can combine all the pieces into our DecisionTree class. Two minor additions have been made. . in the find_varsplit method, we have added a recursive call to build a decision tree on the left and right groups if we have previously found a split and we have not excceeded our maximum tree depth. | for debugging purposes, we added the __repr__ method, that allows us to print out some nicely formated data about our tree in the jupyter environment. | def sum_squares(x_vals,x_sq): return np.sum(x_sq - np.mean(x_vals)**2) class DecisionTree(): def __init__(self,x,y,idxs=None,min_leaf_size=5,max_tree_depth=3): self.x,self.y,self.min_leaf_size,self.max_tree_depth = x,y,min_leaf_size,max_tree_depth self.idxs = np.arange(len(y)) if idxs is None else idxs self.n_rows, self.n_cols = len(self.idxs),self.x.shape[1] self.val = np.mean(self.y[self.idxs]) self.score = float(&#39;inf&#39;) self.find_varsplit() #go through all variables and find a variable split def find_varsplit(self): for i in range(self.n_cols): self.find_better_split(i) if self.score == float(&#39;inf&#39;): return # no split was found if self.max_tree_depth &gt; 0: self.left = DecisionTree(self.x,self.y,self.left_idxs,max_tree_depth=self.max_tree_depth-1) self.right = DecisionTree(self.x,self.y,self.right_idxs,max_tree_depth=self.max_tree_depth-1) # find a split that achieves a lower score than the one we already have def find_better_split(self,i): y = self.y[self.idxs] y_sq = y*y col_data = self.x.iloc[self.idxs,i] for x_val in self.x.iloc[self.idxs,i].unique(): left_ss,right_ss = 0.,0. f_left = col_data &lt;= x_val f_right = col_data &gt; x_val if np.sum(f_left) &gt; 0: left_ss = sum_squares(y[f_left],y_sq[f_left]) if np.sum(f_right) &gt; 0: right_ss = sum_squares(y[f_right],y_sq[f_right]) if self.score &gt; left_ss+right_ss: self.score,self.var,self.split = left_ss+right_ss,i,x_val self.left_idxs,self.right_idxs = self.idxs[f_left],self.idxs[f_right] def __repr__(self): s = f&#39;val: {self.val}, n_rows: {self.n_rows}, n_cols: {self.n_cols}&#39; if self.score != float(&#39;inf&#39;): s+= f&#39;, var: {self.x.columns[self.var]}, split: {self.split}&#39; return s . . Note: This version of find_better_split has a runtime of $O(n^2)$ and can be optimized to be $n log(n)$. We will do the optimization in a later post. For now, let&#39;s convince ourselves that this implementation is en par with sci-kit learn&#39;s implementation by testing and comparing on a synthetic data set. . Testing on a Synthetic Data Set . We can easily create a synthetic data set by drawing random numbers. . np.random.seed(42) x_test = pd.DataFrame(np.random.rand(50,3)) x_test.columns = [&#39;x&#39;+str(i) for i in range(3)] y_test = np.array(x_test[&#39;x0&#39;]+x_test[&#39;x1&#39;]*10+x_test[&#39;x2&#39;]*20) . First, let&#39;s check a regression tree build by sklearn . from sklearn.ensemble import RandomForestRegressor from treeinterpreter import treeinterpreter as ti from sklearn.tree import plot_tree . rf = RandomForestRegressor(n_estimators=1,bootstrap=False,max_depth=2) rf.fit(x_test,y_test) . RandomForestRegressor(bootstrap=False, max_depth=2, n_estimators=1) . Now we can build our regression tree on the data. Note that in this inplementation instantiating the object is equivalent to calling the .fit() method in sklearn. . m = DecisionTree(x_test,y_test,max_tree_depth=2) m . val: 15.353092120783176, n_rows: 50, n_cols: 3, var: x2, split: 0.6842330265121569 . m.left . val: 11.500132207976009, n_rows: 33, n_cols: 3, var: x2, split: 0.4271077886262563 . m.left.left . val: 9.130173727062706, n_rows: 21, n_cols: 3, var: x1, split: 0.5924145688620425 . m.left.right . val: 15.647559549574288, n_rows: 12, n_cols: 3, var: x1, split: 0.31171107608941095 . m.right . val: 22.83236724564415, n_rows: 17, n_cols: 3, var: x1, split: 0.8154614284548342 . m.right.left . val: 20.765127832011157, n_rows: 11, n_cols: 3, var: x0, split: 0.1198653673336828 . m.right.right . val: 26.62230617063798, n_rows: 6, n_cols: 3, var: x2, split: 0.8083973481164611 . Values and samples at the nodes are exactly the same as the sklearn implementation! The split values, however are not identical. I suspect that sklearn uses the mean of the two values closest values between the two groups, whereas our implementation uses the an element of the column as cutoff point. . Next up . How to implement a .predict() method for unseen data and optimizing the time complexity! .",
            "url": "https://blog.manuelmai.com//python/from_scratch/ml/2020/06/28/Decision_Trees_from_Scratch.html",
            "relUrl": "/python/from_scratch/ml/2020/06/28/Decision_Trees_from_Scratch.html",
            "date": " • Jun 28, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Welcome",
            "content": "This is an Experiment . Putting my writing in public is frightening. Not because it&#39;s precious to me, but because it does not exist yet. But communicating my intention to blog puts me on the hook to actually deliver something. Now the pressure starts building. I shouldn&#39;t overthink it, though. I am in control. I can shut this thing down in a second, so nothing to worry about. . Many people I look up to have blogs. Jeremy Howard and Rachel Thomas, Tim Ferris, and Seth Godin all practice blogging for their own good reasons. There are common themes among their reasons for blogging. . You blog for yourself and use writing as a thinking tool. | You add value to others by sharing what you spent time learning and digesting, so you can save them time. | You build an audience and get them thinking about topics and causes you find valuable. | My intention with this blog to capture some of my technical learnings, teachings, and potentially a stream of conciousness. I do not know how frequently I will post or how deeply I will explore certain topics, but I believe it can serve me as a tool to clarify my own thinking, and improve my writing. . My hope is that others, like you, find some portion of this blog useful. Only time will tell if there is any value in this for me or you, so I am not making any big promises. . My commitment is to create at least 5 posts following this one. This is just a test, nothing to lose. It&#39;s just an experiment. .",
            "url": "https://blog.manuelmai.com//streamofconsciousness/2020/06/19/Welcome.html",
            "relUrl": "/streamofconsciousness/2020/06/19/Welcome.html",
            "date": " • Jun 19, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a Physics Ph.D. turned Data Scientist and I’m interested in all things AI. . This is my dog. Her name is Mia. . .",
          "url": "https://blog.manuelmai.com//about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://blog.manuelmai.com//robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}