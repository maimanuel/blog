{
  
    
        "post0": {
            "title": "Decision Trees from Scratch Part 2",
            "content": "Introduction . In the previous post we developed a simple, non-optimized version of a fitting method for a decision tree for regression. In this post we will add a predict method. . Predicting a Sample . We are focused on the regression problem here, so predicting the y value of a sample row x means tracing the tree based on the splits and then predicting self.value once we reach a leaf node. . def predict_row(self,row): # assume &#39;row&#39; is a DF row or numpy array # check if it&#39;s a leaf node, then return mean of y for this node if self.score == float(&#39;inf&#39;): return self.value # Otherwise check variable split and move down the correct branch of the tree t = self.left if row[self.var] &lt;= self.split else self.right return t.predict_row(row) . The predict_row(x) method is written recursively and we start with the behavior at a leaf node, because it&#39;s very easy. If we are at the leaf node, we simply predict sel.val of that node which was set up to be the mean y of all training samples that ended up in this node. If we are not at a leaf node, we need to go deeper down the tree. . The way we do it is to trace down the variable split for this node, which will exist by definition because we are not at a leaf node. The index of the split variable is stored in self.var and the split value in self.split. We now have all the pieces in place to go down the proper branch of the tree. If the split variable x[self.var] is less than or equal to the split value self.split we go to the left sub-tree a call predict_row on that node, otherwise we will go right. . That is all the recursion does. It will go down the tree until eventually we hit a leaf node and we return self.value as the prediction for this particular row x. . Now the final step to do is to implement prediction on an entire DataFrame x. The idea is very simple. The predict method will just return an array of all the predictions for each row. . def predict(self,x): return np.array([self.predict_row(row) for row in x.to_numpy()]) . Completed Class . Let&#39;s take a look at the completed class that we have assembled so far. . def sum_squares(x_vals,x_sq): return np.sum(x_sq - np.mean(x_vals)**2) class DecisionTree(): def __init__(self,x,y,idxs=None,min_leaf_size=5,max_tree_depth=3): self.x,self.y,self.min_leaf_size,self.max_tree_depth = x,y,min_leaf_size,max_tree_depth self.idxs = np.arange(len(y)) if idxs is None else idxs self.n_rows, self.n_cols = len(self.idxs),self.x.shape[1] self.val = np.mean(self.y[self.idxs]) self.score = float(&#39;inf&#39;) self.find_varsplit() #go through all variables and find a variable split def find_varsplit(self): if self.max_tree_depth &gt; 0: for i in range(self.n_cols): self.find_better_split(i) if self.score == float(&#39;inf&#39;): return # no split was found self.left = DecisionTree(self.x,self.y,self.left_idxs,max_tree_depth=self.max_tree_depth-1) self.right = DecisionTree(self.x,self.y,self.right_idxs,max_tree_depth=self.max_tree_depth-1) # find a split that achieves a lower score than the one we already have def find_better_split(self,i): y = self.y[self.idxs] y_sq = y*y col_data = self.x.iloc[self.idxs,i] for x_val in self.x.iloc[self.idxs,i].unique(): left_ss,right_ss = float(&#39;inf&#39;),float(&#39;inf&#39;) f_left = col_data &lt;= x_val f_right = col_data &gt; x_val if np.sum(f_left) &gt; 0: left_ss = sum_squares(y[f_left],y_sq[f_left]) if np.sum(f_right) &gt; 0: right_ss = sum_squares(y[f_right],y_sq[f_right]) if self.score &gt; left_ss+right_ss: self.score,self.var,self.split = left_ss+right_ss,i,x_val self.left_idxs,self.right_idxs = self.idxs[f_left],self.idxs[f_right] # predict y values for a data frame x def predict(self,x): return np.array([self.predict_row(row) for row in x.to_numpy()]) def predict_row(self,row): # assume &#39;row&#39; is a DF row # check if it&#39;s a leaf node, then return mean of y for this node if self.score == float(&#39;inf&#39;): return self.val # Otherwise check variable split and move down the correct branch of the tree t = self.left if row[self.var] &lt;= self.split else self.right return t.predict_row(row) #print a nice representation of the tree instance def __repr__(self): s = f&#39;val: {self.val}, n_rows: {self.n_rows}, n_cols: {self.n_cols}&#39; if self.score != float(&#39;inf&#39;): s+= f&#39;, var: {self.x.columns[self.var]}, split: {self.split}&#39; return s . When we initialize the class with data x,y we start fitting a tree. Now we can use a validation or test set and predict new values y_new from a new input set x_new by calling predict(x_new), pretty cool! Let&#39;s try it out. . Test on Synthetic Data . Generate a synthetic data set and split into train and test sets. . np.random.seed(42) x = pd.DataFrame(np.random.rand(150,3)) x.columns = [&#39;x&#39;+str(i) for i in range(3)] y = np.array(x[&#39;x0&#39;]+x[&#39;x1&#39;]*10+x[&#39;x2&#39;]*20) x_train = x[:100] y_train = y[:100] x_test = x[100:] y_test = y[100:] . DT = DecisionTree(x_train,y_train,max_tree_depth=4) . y_pred = DT.predict(x_test) . Let&#39;s visually inspect the predictions. . plt.scatter(y_test,y_pred) plt.xlabel(&#39;$y_{ rm test}$&#39;) plt.ylabel(&#39;$y_{ rm pred}$&#39;) plt.title(&#39;Prediction vs. Test Labels&#39;) . Text(0.5, 1.0, &#39;Prediction vs. Test Labels&#39;) . This looks like a great success! We have achieved a high correlation, and we will quantify the performance on real data once we optimize the fitting procedure. . Next up . Optimizing the time complexity and assembling the Random Forest. . Photo Credit Preview Photo by Michael Dziedzic on Unsplash .",
            "url": "https://blog.manuelmai.com//python/from_scratch/ml/2020/07/11/Decision_Trees_from_Scratch_Part_2.html",
            "relUrl": "/python/from_scratch/ml/2020/07/11/Decision_Trees_from_Scratch_Part_2.html",
            "date": " • Jul 11, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Decision Trees from Scratch",
            "content": "Introduction . Decision Trees form an important basis for one of the most popular and useful supervised machine learning algorithms of modern times, the Random Forest. In this post I will develop a very basic, non-optimized version of a decision tree for regression. I will optimize the tree and build a random forest from multiple trees in a later post. To say this post is inspired by fastai&#39;s machine learning course is an understatement. I have watched the videos and follow Jeremy&#39;s code closely. . How Does it Work? - Predict the Mean! . The goal of any supervised machine learning algorithm is to use a set of input features $X$ and map them as closely to a set of targets $y$. In other words, the task is to find a function $f$ such that $$f(X) approx y.$$ A decision tree is one of many algorithms that allow us to effectively find a good function $f$. The basic idea is the following. If we wanted to create a trivial model that predicted the exact same value for all rows of $X$, which value should we choose? That depends on how we measure how close our prediction is to the actual targets $y$! It is very common to try to minimize the mse or mean squared error of our prediction $f(X)$. The means squared error is defined as $${ rm MSE}(y^{ rm true},y^{ rm pred}) = frac{1}{n} sum_{i=1}^{n}(y_i^{ rm true} -y_i^{ rm pred})^2,$$ . where the predictions are the model output $y^{ rm pred} = f(X)$. If we build a model where all our predictions are the same, there is one specific value that would minimize the mean squared error and that value is the mean of the true labels for every single row of $X$. . Where is the Tree? . Predicting the mean of the target variable is a good approximation, but we can obviously do better. We will now do split the rows into two groups, and then predict the mean of $y^{ rm true}$ for each of the separate groups. And then we repeat on the subgroups and split those again, and so on... . Why should that be better? Well, consider the case were we have split the rows to the point where each row gets their own prediction. In that case there would be no error at all, but the model would be very specific to the dataset, i.e., if you are row $i$, we predict $y_i^{ rm true}$. This model would be overfit to the data. . In general, we want to be somewhere in between the extremes, predict more than one value for the entire dataset, but predict fewer values than the exact ones that are given in the training set. In order to get there, we split the data some number of times, how often is controlled by a hyperparameter of the model. . The decision tree is hierarchical structure of splits that divide the dataset into groups, such that each element of the new groups can be better approximated by the group mean of targets, than by the mean of targets of the combined groups before the split. . How to Choose the Splits? . To find the splits we will simply iterate through all columns and for each column check the range of all values in the columns as a cutoff point. For example, assume we want to predict a persons weight from the two columns age and height. First, check column age, and try out all possible values of ages that are found in the column. . Let&#39;s say there are values 10, 55, 68. For each of those values check if using the mean weight of the group produces a lower error than predicting just one weight for everyone. In this case there are only two groupings available, everyone with age $ leq 10$ and everyone with age $ leq 55$. It seems likely, that the persons with age 55, and 68 have a weight closer to each other than a child (or several children) of age 10. If the suspicion is true, we will keep variable age and split value 55 as candidates for the first split. . Next up, we will check all values of the column height and try to achieve a lower error than our candidate split from column age. If we cannot find a split that produces a lower error, we will keep the candidate split, otherwise we will use the newly found better one. We can now repeat that procedure with the same columns on the subgroups, until we hit a stopping criterion. This could be max number of splits = tree depth has bee reached, or we would require a minimum number of rows per group and we cannot split anymore if we would have too few rows in a group. . This is basically the entire algorithm! Once we have set up the tree with all the split columns and values, we can send new and unseen data through the tree, e.g., if age &lt;= 55 go right, if height &gt; 5&#39;8&#39;&#39; go left then predict group mean weight = 185lbs. . Let&#39;s Code Up a Basic Version . First lines of code to initialize the base class. . class DecisionTree(): def __init__(self,x,y,idxs=None,min_leaf_size=5,max_tree_depth=3): self.x,self.y,self.idxs,self.min_leaf_size,self.max_tree_depth = x,y,idxs,min_leaf_size,max_tree_depth . To initialize a tree we will pass it some basic information. The data $x$, in our case a pandas DataFrame, the prediction target, or labels, $y$ which will be a numpy array, and possibly some other parameters, such as min_leaf_size, and max_tree_depth. In case we are not at the root node, we will also need to pass a set of indices to refer to the rows, which made it into this particular node a after previous split. . If we are at the root node, then the rows to consider to split on are actually all rows. Hence we can simply construct a full set of idxs in case the value is None. . class DecisionTree(): def __init__(self,x,y,idxs=None,min_leaf_size=5,max_tree_depth=3): self.x,self.y,self.min_leaf_size,self.max_tree_depth = x,y,min_leaf_size,max_tree_depth self.idxs = np.arange(len(y)) if idxs is None else idxs self.n_rows, self.n_cols = len(self.idxs),self.x.shape[1] self.val = np.mean(self.y[self.idxs]) self.score = float(&#39;inf&#39;) self.find_varsplit() . Next up is the computation of the prediction value for this node, which is simply the mean of all $y$s that made it to the node. Since we want to minimize the error score, we&#39;ll also keep track of that and initialize it to $ infty$. Instead of the MSE, we&#39;re tracking the total sum of squares, to go around thinking about computing weighted averages. Finally, we want to find out for on which variable (if any) we will split next, and at which lavel of that variable we will split the remaining data. . Let&#39;s try to figure out how to do the splitting. . class DecisionTree(): def __init__(self,x,y,idxs=None,min_leaf_size=5,max_tree_depth=3): ... self.find_varsplit() #go through all variables and find a variable split def find_varsplit(self): for i in range(n_cols): self.find_better_split(i) # find a split that achieves a lower score than the one we already have def find_better_split(self,i): pass . The idea here is that once we are sure that we want another split, we iterate through all variables (columns), find the best split for that variable, and compare that to self.score. If the newly found best split on variable i is better than the new best one so far, we&#39;ll keep it as the best, otherwise we move on. . def sum_squares(x_vals,x_sq): return np.sum(x_sq - np.mean(x_vals)**2) def find_better_split(self,i): y = self.y[self.idxs] y_sq = y*y col_data = self.x.iloc[self.idxs,i] for x_val in self.x.iloc[self.idxs,i].unique(): left_ss,right_ss = 0.,0. f_left = col_data &lt;= x_val f_right = col_data &gt; x_val if np.sum(f_left) &gt; 0: left_ss = sum_squares(y[f_left],y_sq[f_left]) if np.sum(f_right) &gt; 0: right_ss = sum_squares(y[f_right],y_sq[f_right]) if self.score &gt; left_ss+right_ss: self.score,self.var,self.split = left_ss+right_ss,i,x_val self.left_idxs,self.right_idxs = self.idxs[f_left],self.idxs[f_right] . This function goes through a particular column, indexed by i, and splits the data by all possible unique values that are found in the column. For each of the splits, left and right, we compute the sum of squared errors when predicting the group mean of the target variable. We then check if this is the best split found so far. If it is, we keep the relevant information, indices for left and right groups, column index, and split value. . Putting it All Together . Now we can combine all the pieces into our DecisionTree class. Two minor additions have been made. . in the find_varsplit method, we have added a recursive call to build a decision tree on the left and right groups if we have previously found a split and we have not excceeded our maximum tree depth. | for debugging purposes, we added the __repr__ method, that allows us to print out some nicely formated data about our tree in the jupyter environment. | def sum_squares(x_vals,x_sq): return np.sum(x_sq - np.mean(x_vals)**2) class DecisionTree(): def __init__(self,x,y,idxs=None,min_leaf_size=5,max_tree_depth=3): self.x,self.y,self.min_leaf_size,self.max_tree_depth = x,y,min_leaf_size,max_tree_depth self.idxs = np.arange(len(y)) if idxs is None else idxs self.n_rows, self.n_cols = len(self.idxs),self.x.shape[1] self.val = np.mean(self.y[self.idxs]) self.score = float(&#39;inf&#39;) self.find_varsplit() #go through all variables and find a variable split def find_varsplit(self): if self.max_tree_depth &gt; 0: for i in range(self.n_cols): self.find_better_split(i) if self.score == float(&#39;inf&#39;): return # no split was found self.left = DecisionTree(self.x,self.y,self.left_idxs,max_tree_depth=self.max_tree_depth-1) self.right = DecisionTree(self.x,self.y,self.right_idxs,max_tree_depth=self.max_tree_depth-1) # find a split that achieves a lower score than the one we already have def find_better_split(self,i): y = self.y[self.idxs] y_sq = y*y col_data = self.x.iloc[self.idxs,i] for x_val in self.x.iloc[self.idxs,i].unique(): left_ss,right_ss = 0.,0. f_left = col_data &lt;= x_val f_right = col_data &gt; x_val if np.sum(f_left) &gt; 0: left_ss = sum_squares(y[f_left],y_sq[f_left]) if np.sum(f_right) &gt; 0: right_ss = sum_squares(y[f_right],y_sq[f_right]) if self.score &gt; left_ss+right_ss: self.score,self.var,self.split = left_ss+right_ss,i,x_val self.left_idxs,self.right_idxs = self.idxs[f_left],self.idxs[f_right] def __repr__(self): s = f&#39;val: {self.val}, n_rows: {self.n_rows}, n_cols: {self.n_cols}&#39; if self.score != float(&#39;inf&#39;): s+= f&#39;, var: {self.x.columns[self.var]}, split: {self.split}&#39; return s . . Note: This version of find_better_split has a runtime of $O(n^2)$ and can be optimized to be $n log(n)$. We will do the optimization in a later post. For now, let&#39;s convince ourselves that this implementation is en par with sci-kit learn&#39;s implementation by testing and comparing on a synthetic data set. . Testing on a Synthetic Data Set . We can easily create a synthetic data set by drawing random numbers. . np.random.seed(42) x_test = pd.DataFrame(np.random.rand(50,3)) x_test.columns = [&#39;x&#39;+str(i) for i in range(3)] y_test = np.array(x_test[&#39;x0&#39;]+x_test[&#39;x1&#39;]*10+x_test[&#39;x2&#39;]*20) . First, let&#39;s check a regression tree build by sklearn . from sklearn.ensemble import RandomForestRegressor from treeinterpreter import treeinterpreter as ti from sklearn.tree import plot_tree . rf = RandomForestRegressor(n_estimators=1,bootstrap=False,max_depth=2) rf.fit(x_test,y_test) . RandomForestRegressor(bootstrap=False, max_depth=2, n_estimators=1) . Now we can build our regression tree on the data. Note that in this inplementation instantiating the object is equivalent to calling the .fit() method in sklearn. . m = DecisionTree(x_test,y_test,max_tree_depth=2) m . val: 15.353092120783176, n_rows: 50, n_cols: 3, var: x2, split: 0.6842330265121569 . m.left . val: 11.500132207976009, n_rows: 33, n_cols: 3, var: x2, split: 0.4271077886262563 . m.left.left . val: 9.130173727062706, n_rows: 21, n_cols: 3 . m.left.right . val: 15.647559549574288, n_rows: 12, n_cols: 3 . m.right . val: 22.83236724564415, n_rows: 17, n_cols: 3, var: x1, split: 0.8154614284548342 . m.right.left . val: 20.765127832011157, n_rows: 11, n_cols: 3 . m.right.right . val: 26.62230617063798, n_rows: 6, n_cols: 3 . Values and samples at the nodes are exactly the same as the sklearn implementation! The split values, however are not identical. I suspect that sklearn uses the mean of the two values closest values between the two groups, whereas our implementation uses the an element of the column as cutoff point. . Next up . How to implement a .predict() method for unseen data and optimizing the time complexity! .",
            "url": "https://blog.manuelmai.com//python/from_scratch/ml/2020/06/28/Decision_Trees_from_Scratch.html",
            "relUrl": "/python/from_scratch/ml/2020/06/28/Decision_Trees_from_Scratch.html",
            "date": " • Jun 28, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Welcome",
            "content": "This is an Experiment . Putting my writing in public is frightening. Not because it&#39;s precious to me, but because it does not exist yet. But communicating my intention to blog puts me on the hook to actually deliver something. Now the pressure starts building. I shouldn&#39;t overthink it, though. I am in control. I can shut this thing down in a second, so nothing to worry about. . Many people I look up to have blogs. Jeremy Howard and Rachel Thomas, Tim Ferris, and Seth Godin all practice blogging for their own good reasons. There are common themes among their reasons for blogging. . You blog for yourself and use writing as a thinking tool. | You add value to others by sharing what you spent time learning and digesting, so you can save them time. | You build an audience and get them thinking about topics and causes you find valuable. | My intention with this blog to capture some of my technical learnings, teachings, and potentially a stream of conciousness. I do not know how frequently I will post or how deeply I will explore certain topics, but I believe it can serve me as a tool to clarify my own thinking, and improve my writing. . My hope is that others, like you, find some portion of this blog useful. Only time will tell if there is any value in this for me or you, so I am not making any big promises. . My commitment is to create at least 5 posts following this one. This is just a test, nothing to lose. It&#39;s just an experiment. .",
            "url": "https://blog.manuelmai.com//streamofconsciousness/2020/06/19/Welcome.html",
            "relUrl": "/streamofconsciousness/2020/06/19/Welcome.html",
            "date": " • Jun 19, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a Physics Ph.D. turned Data Scientist and I’m interested in all things AI. . This is my dog. Her name is Mia. . .",
          "url": "https://blog.manuelmai.com//about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://blog.manuelmai.com//robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}